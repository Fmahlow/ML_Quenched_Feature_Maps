{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação de Gradient Boosting com e sem features quânticas\n",
    "\n",
    "Este notebook carrega os *folds* disponíveis na pasta `features/`, treina modelos de Gradient Boosting usando apenas as features clássicas (`class_0` a `class_12`) e o conjunto combinado de features clássicas + quânticas (`qf_0` a `qf_12`). Em seguida, comparamos o desempenho entre os dois cenários e geramos as predições solicitadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependências\n",
    "\n",
    "O notebook utiliza `pandas`, `numpy`, `scikit-learn`, `matplotlib` e `seaborn`. Caso ainda não as tenha instalado no seu ambiente, execute o comando abaixo em uma célula separada ou diretamente no terminal:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Configuração estética padrão para os gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.figsize\": (10, 5), \"axes.titlesize\": 14, \"axes.labelsize\": 12})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados\n",
    "\n",
    "Cada CSV em `features/` corresponde a um *fold*. O conjunto possui uma coluna `set` indicando se a amostra pertence ao treino ou ao teste.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = Path(\"features\")\n",
    "fold_paths = sorted(data_dir.glob(\"features_y_fold*.csv\"))\n",
    "if not fold_paths:\n",
    "    raise FileNotFoundError(\"Nenhum arquivo `features_y_fold*.csv` foi encontrado na pasta `features/`.\")\n",
    "\n",
    "fold_frames = []\n",
    "for path in fold_paths:\n",
    "    frame = pd.read_csv(path)\n",
    "    frame[\"fold_name\"] = path.stem\n",
    "    fold_frames.append(frame)\n",
    "\n",
    "fold_summary = (\n",
    "    pd.concat(\n",
    "        [df.assign(set=df[\"set\"].str.lower())[[\"fold\", \"fold_name\", \"set\"]] for df in fold_frames],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "fold_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição dos grupos de features e função de avaliação\n",
    "\n",
    "A função abaixo treina um `GradientBoostingClassifier` para cada conjunto de features em todos os *folds*, calcula as métricas desejadas no conjunto de teste e armazena as predições geradas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "classical_features = [col for col in fold_frames[0].columns if col.startswith(\"class_\")]\n",
    "quantum_features = [col for col in fold_frames[0].columns if col.startswith(\"qf_\")]\n",
    "\n",
    "feature_sets = {\n",
    "    \"Benchmark\": classical_features,\n",
    "    \"Quantum\": classical_features + quantum_features,\n",
    "}\n",
    "\n",
    "metric_functions = {\n",
    "    \"AUC\": lambda y_true, y_score, y_pred: roc_auc_score(y_true, y_score) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "    \"F1 Score Overall\": lambda y_true, y_score, y_pred: f1_score(y_true, y_pred),\n",
    "    \"Balanced Accuracy\": lambda y_true, y_score, y_pred: balanced_accuracy_score(y_true, y_pred),\n",
    "    \"Precision Class 0\": lambda y_true, y_score, y_pred: precision_score(y_true, y_pred, pos_label=0),\n",
    "    \"Precision Class 1\": lambda y_true, y_score, y_pred: precision_score(y_true, y_pred, pos_label=1),\n",
    "    \"Recall Class 0\": lambda y_true, y_score, y_pred: recall_score(y_true, y_pred, pos_label=0),\n",
    "    \"Recall Class 1\": lambda y_true, y_score, y_pred: recall_score(y_true, y_pred, pos_label=1),\n",
    "}\n",
    "\n",
    "results = []\n",
    "prediction_frames = []\n",
    "\n",
    "for fold_idx, fold_df in enumerate(fold_frames):\n",
    "    train_df = fold_df[fold_df[\"set\"] == \"train\"]\n",
    "    test_df = fold_df[fold_df[\"set\"] == \"test\"]\n",
    "\n",
    "    y_train = train_df[\"y\"]\n",
    "    y_test = test_df[\"y\"]\n",
    "\n",
    "    for label, columns in feature_sets.items():\n",
    "        X_train = train_df[columns]\n",
    "        X_test = test_df[columns]\n",
    "\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        for metric_name, metric_fn in metric_functions.items():\n",
    "            value = metric_fn(y_test, y_proba, y_pred)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"fold_name\": fold_df[\"fold_name\"].iat[0],\n",
    "                    \"model\": label,\n",
    "                    \"metric\": metric_name,\n",
    "                    \"value\": value,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        prediction_frames.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"fold_name\": fold_df[\"fold_name\"].iat[0],\n",
    "                    \"row_id\": test_df[\"row_id\"].values,\n",
    "                    \"y_true\": y_test.values,\n",
    "                    \"model\": label,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"y_proba\": y_proba,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "predictions_df = pd.concat(prediction_frames, ignore_index=True)\n",
    "\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo das métricas por modelo\n",
    "\n",
    "A tabela a seguir mostra a mediana e o intervalo interquartil (25%-75%) das métricas em todos os *folds*.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_summary = (\n",
    "    results_df\n",
    "    .groupby(['model', 'metric'])['value']\n",
    "    .agg(\n",
    "        median=lambda s: np.nanmedian(s),\n",
    "        q1=lambda s: np.nanquantile(s, 0.25),\n",
    "        q3=lambda s: np.nanquantile(s, 0.75),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "metrics_summary['iqr'] = metrics_summary['q3'] - metrics_summary['q1']\n",
    "metrics_summary['median_iqr'] = metrics_summary.apply(\n",
    "    lambda row: f\"{row['median']:.4f} [{row['q1']:.4f}, {row['q3']:.4f}]\",\n",
    "    axis=1,\n",
    ")\n",
    "metrics_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráfico comparativo\n",
    "\n",
    "O gráfico reproduz o exemplo solicitado, com barras pretas representando o benchmark (apenas features clássicas) e barras amarelas representando o modelo com features clássicas + quânticas. Os valores exibidos nas barras correspondem às medianas por *fold*.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_order = [\n",
    "    'AUC',\n",
    "    'F1 Score Overall',\n",
    "    'Balanced Accuracy',\n",
    "    'Precision Class 0',\n",
    "    'Precision Class 1',\n",
    "    'Recall Class 0',\n",
    "    'Recall Class 1',\n",
    "]\n",
    "\n",
    "plot_ready = (\n",
    "    metrics_summary\n",
    "    .pivot(index='metric', columns='model', values='median')\n",
    "    .reindex(metric_order)\n",
    ")\n",
    "\n",
    "ax = plot_ready.plot(\n",
    "    kind='bar',\n",
    "    color={'Benchmark': '#2e2e2e', 'Quantum': '#f1b82d'},\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('')\n",
    "ax.set_title('Toxicity classification with Gradient Boosting')\n",
    "ax.legend(title='Modelo')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', label_type='edge', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predições\n",
    "\n",
    "As tabelas abaixo exibem, respectivamente, as primeiras linhas das predições de teste para o modelo benchmark e para o modelo com features quânticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "benchmark_predictions = predictions_df[predictions_df['model'] == 'Benchmark']\n",
    "quantum_predictions = predictions_df[predictions_df['model'] == 'Quantum']\n",
    "\n",
    "benchmark_predictions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantum_predictions.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}